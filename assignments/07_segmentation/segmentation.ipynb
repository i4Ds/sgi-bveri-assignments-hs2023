{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89973334",
   "metadata": {},
   "source": [
    "## FHNW bverI - HS2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39280e4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f10f19697935a18db955753fdc25539f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a66f2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "806a76a1c0d98df891ae2b541f3eeea5",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Segmentation\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- Semantic Segmentation: Architektur-Design, Trainieren und Evaluieren von Modellen\n",
    "- Upsampling: Techniken kennen und anwenden\n",
    "- Instance Segmentation: Anwenden von Pre-Trained Modellen, Verstehen & Evaluieren der Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e3d85-8fd0-465d-acc1-3ce127b77bd0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Im Folgenden installieren und laden wir die benötigten Python packages. Danach setzten wir die Pfade für den Zugriff auf Daten und spezifizieren einen Output-Folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a538d-1af1-46b8-a5f1-1dad9bcad1c6",
   "metadata": {},
   "source": [
    "Mount your google drive / define a data path to store data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29a376-4f08-4934-97bd-fd31e23e64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6c669-c7f6-445f-907c-4c2b9adb36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18762b-6c40-4aa0-b8c9-e507bb6aca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ade056-826d-4a52-a1c4-e8dd7491491d",
   "metadata": {},
   "source": [
    "Modifizieren Sie die folgenden Pfade bei Bedarf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a3048-2cb8-4938-a61e-21bfaaadbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DATA_PATH = Path('/content/drive/MyDrive/bverI/data')\n",
    "else:\n",
    "    DATA_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd997a-5213-486a-8f83-c0c8289cffd7",
   "metadata": {},
   "source": [
    "Install packages not in base Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39df204-5cae-4073-9238-d76cd16c259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.system(\"pip install torchshow torchinfo gdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458ede3-41f5-44c2-b2b6-8234f84534ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f7c7814eef97f35e012b0317b5ab81d",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchshow as ts\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0544f-330b-430e-9075-ef4e8209edcb",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "Hier werden Sie ein _Fully-Convolutional Network_ trainieren für _Semantic Segmentation_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b1ca8-31c6-4120-98de-20f586f6a227",
   "metadata": {},
   "source": [
    "### Stanford Background Dataset\n",
    "\n",
    "Wir schauen uns das [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html) an. Dort gibt es verschiedene Szenerien mit _semantic segmentation_ Annotationen.\n",
    "\n",
    "Als erstes laden wir den Datensatz runter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918df1b-ec82-4f1f-a53b-26ec4fecd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "file_id = \"1bXWW8v-vASZ6dUv2CchhrbvyQU4uE2dk\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "download_path = DATA_PATH / \"stanford_background_dataset.zip\"\n",
    "if not download_path.exists():\n",
    "    gdown.download(url, str(download_path), quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06e43a-1031-422a-9782-e7c45def3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (DATA_PATH / \"stanford_background_dataset\").exists():\n",
    "    CMD = f\"unzip {str(download_path)} -d {DATA_PATH}\"\n",
    "    os.system(CMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13d30c-a566-485c-90a7-46058d45fccd",
   "metadata": {},
   "source": [
    "Nun schauen wir uns eines der Bilder an:\n",
    "\n",
    "DATA_PATH.joinpath(\"stanford_background_dataset/images/0000047.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef9d0e-fa1d-4f07-bcf4-bea851f6127e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_sbds = Image.open(\n",
    "    DATA_PATH.joinpath(\"stanford_background_dataset/images/0000047.jpg\"))\n",
    "label_path = DATA_PATH.joinpath(\n",
    "    \"stanford_background_dataset/labels/0000047.regions.txt\")\n",
    "display(img_sbds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfb291-5cf4-4a8e-96e9-1eb9b2ce63f1",
   "metadata": {},
   "source": [
    "Wir definieren nun ein `torch.utils.data.Dataset` um ein Bild und dessen _Segmentation Map_ auszugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135dc926-cb8d-4839-8187-02b1ef3e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class StanfordBackgroundDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the Stanford Background Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_path: Path, transform=None, transform_labels=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            root_path (Path): Path to the dataset directory.\n",
    "            transform (callable, optional): Transformation function for images.\n",
    "            transform_labels (callable, optional): Transformation function for labels.\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        self.transform_labels = transform_labels\n",
    "        self.image_paths = list(root_path.joinpath(\"images\").glob(\"*.jpg\"))\n",
    "        self.classes = [\"sky\", \"tree\", \"road\", \"grass\", \"water\", \"building\", \"mountain\", \"foreground object\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of items in the dataset. \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item by its index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the image, label mask, and label image.\n",
    "        \"\"\"\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = np.array(Image.open(image_path))\n",
    "\n",
    "        label_path = self.root_path.joinpath(f\"labels/{image_path.stem}.regions.txt\")\n",
    "        labels = self._parse_regions(label_path)\n",
    "\n",
    "        mask = torch.zeros(len(self.classes), *labels.shape)\n",
    "        labels_tensor = torch.tensor(labels).unsqueeze(0)\n",
    "        labels_clipped = torch.clip(labels_tensor, 0, len(self.classes) - 1)\n",
    "        label_masks = mask.scatter_(0, labels_clipped, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform_labels:\n",
    "            label_masks = self.transform_labels(label_masks)\n",
    "            labels_tensor = self.transform_labels(labels_tensor)\n",
    "\n",
    "        return image, label_masks, labels_tensor\n",
    "\n",
    "    def _parse_regions(self, path):\n",
    "        \"\"\"\n",
    "        Parses the region labels from a file.\n",
    "\n",
    "        Args:\n",
    "            path (Path): Path to the label file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of labels.\n",
    "        \"\"\"\n",
    "        with open(path, \"r\") as file:\n",
    "            lines = [list(map(int, line.split())) for line in file]\n",
    "        return np.array(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19abebf-00c0-4b67-a89c-3a86d66dd85d",
   "metadata": {},
   "source": [
    "Instanzieren Sie das Dataset mit der Klasse `StanfordBackgroundDataset`. Wählen Sie danach die erste Beobachtung aus und visualisieren Sie: Bild, Segmentation Map und Masken. Verwenden Sie `torchshow.show`.\n",
    "\n",
    "Überprüfen Sie ob die Daten korrekt aussehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266cad3-8527-4276-934c-017956fa4a48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df0099edd434946cde8712fa681710b6",
     "grade": true,
     "grade_id": "dataset",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_path = DATA_PATH.joinpath(\"stanford_background_dataset\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0826ee-92b3-4c2a-8a33-0a28f99b2fb8",
   "metadata": {},
   "source": [
    "### Fully-Convolutional Network\n",
    "\n",
    "Implementieren Sie ein FCN mit einer Encoder-Decoder Architektur. Ergänzen Sie die Klassen entsprechend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217a141-4328-4884-82ee-3c5be0d432ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4151753a83c0addd5ecb4d258069853e",
     "grade": true,
     "grade_id": "fcn",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\" Encoder-Decoder \"\"\"\n",
    "    def __init__(self, encoder, decoder, num_initial_channels, num_input_channels, num_output_channels):\n",
    "        super().__init__()\n",
    "        self.input = nn.Conv2d(3, num_initial_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output = nn.Conv2d(num_input_channels,  num_output_channels, kernel_size=(1, 1), stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        # Transposed Convolution Layer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for i, (in_channels, out_channels) in enumerate(zip(num_channels, num_channels[1:])):\n",
    "            self.layers_.append(EncoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for i, (in_channels, out_channels) in enumerate(zip(num_channels, num_channels[1:])):\n",
    "            self.layers_.append(DecoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9aa9e-03a1-4c1e-8b78-baf1cef9e9c8",
   "metadata": {},
   "source": [
    "Überprüfen Sie die Architektur. Z.B. das die Output-Shape korrekt ist. Wir möchten pro Klasse eine eigene Maske erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c377-77ac-417b-b3b6-e70df02b8698",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9016bd0d276965831b14494a87c69233",
     "grade": true,
     "grade_id": "model",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((200, 300)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])\n",
    "\n",
    "transf_labels = transforms.Compose([\n",
    "    transforms.CenterCrop((200, 300))\n",
    "])\n",
    "\n",
    "ds = StanfordBackgroundDataset(root_path, transform=transf)\n",
    "\n",
    "example_image, example_masks, example_labels = ds[0]\n",
    "\n",
    "# Create / instantiate your model and process an example image with it\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192bf32-1ba3-4a0d-945c-ac22032fb1e5",
   "metadata": {},
   "source": [
    "### Model-Training und Metriken\n",
    "\n",
    "Nun werden wir das Modell trainieren und den Fortschritt monitoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fdccd-c69f-438a-ab6a-503189e638db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((200, 300)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])\n",
    "\n",
    "transf_labels = transforms.Compose([\n",
    "    transforms.CenterCrop((200, 300))\n",
    "])\n",
    "\n",
    "ds = StanfordBackgroundDataset(root_path, transform=transf, transform_labels=transf_labels)\n",
    "\n",
    "ds_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6363f-ed42-4a47-87f9-33c0de47e1a4",
   "metadata": {},
   "source": [
    "Ergänzen Sie den Trainings-Loop wo nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b36226-e241-4d4e-938d-672ef6615fd6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8fdd6873e8b9f79014a015d61c9cf68",
     "grade": true,
     "grade_id": "train",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 8\n",
    "\n",
    "# create model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Create Loss-Function and Optimizer\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "pbar = tqdm(total=num_epochs * len(ds_loader))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(ds_loader):\n",
    "        \n",
    "        images, label_masks, label_images = data\n",
    "        \n",
    "        # Forward-Pass\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Optimize\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Calculate Pixel-Accuracy\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_acc += pixel_acc\n",
    "        step += 1\n",
    "        print_every = 10\n",
    "        if (i % print_every) == (print_every - 1):\n",
    "            desc = f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_every:.3f} acc: {running_acc / print_every:.3f}'\n",
    "            _ = pbar.update(print_every)\n",
    "            _ = pbar.set_description(desc)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "pbar.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334187-7e84-499f-9c67-bd763d6ea59e",
   "metadata": {},
   "source": [
    "Visualisieren Sie die Vorhersage auf einem Bild und vergleichen Sie mit der annotierten _Segmentation map_.\n",
    "\n",
    "Sie können für die Visualisierung `torchshow` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab730b9c-e4ac-4016-ba19-6c36da815361",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62a4de3445a648e6fea89cd5a13ee2fc",
     "grade": true,
     "grade_id": "pred",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d3a6d-4dc7-4b4e-abe3-ab515fa2ce01",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "Hier schauen wir verschiedene Upsampling Techniken an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052fb05-1493-4507-9c98-355da913483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upsample = torch.tensor(\n",
    "    [[1, 2], [3, 4]]).unsqueeze(0).to(torch.float)\n",
    "\n",
    "to_upsample_2 = torch.concat(\n",
    "    [to_upsample, to_upsample], dim=2)\n",
    "\n",
    "to_upsample_2 = torch.concat(\n",
    "    [to_upsample_2, to_upsample_2], dim=1)\n",
    "\n",
    "    \n",
    "def display_arrays(arrays: list[np.ndarray], titles: list[str]):\n",
    "    \"\"\" Display Arrays \"\"\"\n",
    "    num_arrays = len(arrays)\n",
    "    kwargs = {'annot': True, 'cbar': False, 'vmin': 0, 'vmax': 10, 'xticklabels': False, 'yticklabels': False}\n",
    "    fig, ax = plt.subplots(figsize=(3 * num_arrays, 3), ncols=num_arrays)\n",
    "    \n",
    "    # handle single and multi-array plots\n",
    "    if num_arrays > 1:\n",
    "        axes = ax.flatten()\n",
    "    else:\n",
    "        axes = [ax]\n",
    "    \n",
    "    for i, (array, title) in enumerate(zip(arrays, titles)):\n",
    "        sns.heatmap(array, **kwargs, ax=axes[i]).set(\n",
    "            title=f\"{title} - Shape {array.shape}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "display_arrays([np.array(to_upsample[0, :, :])], [\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a743e-9145-4cc8-b3f7-775913163d79",
   "metadata": {},
   "source": [
    "### Unpooling\n",
    "\n",
    "Testen Sie Max-Pooling und Max-Unpooling mit Switch. Probieren Sie verschiedene Parameter aus und schauen Sie sich das Ergebnis an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790330a-c843-414e-925d-618752237788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "to_upsample_3 = torch.clone(to_upsample_2)\n",
    "to_upsample_3[0, 0, 0] = 16\n",
    "\n",
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "output, indices = pool(to_upsample_3)\n",
    "unpooled = unpool(output, indices)\n",
    "\n",
    "display_arrays(arrays=[\n",
    "    np.array(to_upsample_3[0, :, :]),\n",
    "    np.array(output[0, :, :]),\n",
    "    np.array(unpooled[0, :, :])],\n",
    "    titles=[\"input\", \"pooled\", \"unpooled\"]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2145386-bcff-4841-8e88-ee0db8c113e7",
   "metadata": {},
   "source": [
    "### Transpose Convolution\n",
    "\n",
    "Testen Sie verschiedene Parameter für die _Transposed Convolution_. Erstellen Sie 2 weitere Varianten und visualisieren Sie die Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97d909-5acb-4839-9b30-64d67c938c80",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ee4119c16e7f6c9bcbfec934900d7e9",
     "grade": true,
     "grade_id": "trans_conv",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "weight = torch.tensor([[1, 2, 3], [0, 1, 2], [0, 1, 2]]).unsqueeze(0).unsqueeze(0).to(torch.float)\n",
    "weight.shape\n",
    "\n",
    "input_ = to_upsample_2\n",
    "\n",
    "out = F.conv_transpose2d(\n",
    "    input=input_,\n",
    "    weight=weight,\n",
    "    stride=2,\n",
    "    padding=0,\n",
    "    output_padding=0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "arrays_to_plot = [np.array(x) for x in [\n",
    "    input_[0, : :], weight[0, 0, : :], out[0, : :], out2[0, : :], out3[0, : :]]]\n",
    "\n",
    "display_arrays(\n",
    "    arrays=arrays_to_plot,\n",
    "    titles=[\"Input\", \"Filter\", \"Output\", \"Output2\", \"Output3\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d8d49-4072-4e21-aded-9b0889cda439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12a230199dd90ddc3582fc4bc4567aa9",
     "grade": false,
     "grade_id": "instance",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Instance Segmentation mit _Mask R-CNN_\n",
    "\n",
    "In dieser Aufgabe wenden Sie Instance Segmentation an indem Sie ein vortrainiertes Modell verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eea77-6e98-4b75-bebb-13312fe5cb4b",
   "metadata": {},
   "source": [
    "Als erstes laden wir 2 Bilder um Instance Segmentation auszuprobieren. Danach lesen wir die Bilder ein mi `Pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c793d-7148-4f62-a412-f354a8f2b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    {'id': '18zuHwfojUUpmkrQttEtuaNW-MQ0QOoAH',  'name': 'ducks.jpg'},\n",
    "    {'id': '1-UWVWqTpE80Qxh36hPuKkuQZj5BT3hXr', 'name': 'dogs.jpg'}\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    url = f\"https://drive.google.com/uc?id={file['id']}\"\n",
    "    download_path = DATA_PATH / file['name']\n",
    "    if not download_path.exists():\n",
    "        gdown.download(url, str(download_path), quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ada36-4d57-4c34-99c5-92d9f1f46d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dogs = Image.open(DATA_PATH.joinpath(\"dogs.jpg\"))\n",
    "img_ducks = Image.open(DATA_PATH.joinpath(\"ducks.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d609668-c17f-4021-bd3f-107357250e15",
   "metadata": {},
   "source": [
    "Wenden Sie ein vortrainiertes Modell der _Mask R-CNN_ Familie von [torchvision](https://pytorch.org/vision/stable/models.html#instance-segmentation) an. Initialisieren Sie das Modell und führen Sie einen Forward Pass aus über die beiden Beispiel-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031a948-f019-4145-a8da-63bc039e93d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acecb9d07167f1d948b8c5e3dfc8aa0b",
     "grade": true,
     "grade_id": "cell-be98073989cf4c41",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8813b-19f1-438c-aaf1-fd01a750c3e6",
   "metadata": {},
   "source": [
    "Inspizieren Sie den Output von _Mask R-CNN_. Wieviele Objekte hat das Modell gefunden? Was wird alles ausgegeben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2e497-ca62-4f2d-a6dd-9157bbd219e7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f93e184bc906a5f8dd17693cbac8032",
     "grade": true,
     "grade_id": "cell-1fbbeeed9c0a6bdd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1f204-66db-49c3-bac3-86201e86e0e2",
   "metadata": {},
   "source": [
    "Zeichnen Sie die gefundenen Objekte auf das Bild. Probieren Sie verschiedene Werte für `score_threshold` und `proba_threshold`. Was bedeuten die Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c28b30-5643-46c6-9136-43c0315845cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "score_threshold = .75\n",
    "proba_threshold = 0.5\n",
    "\n",
    "boolean_masks = [\n",
    "    out['masks'][out['scores'] > score_threshold] > proba_threshold\n",
    "    for out in output\n",
    "]\n",
    "\n",
    "images_with_masks = [\n",
    "    draw_segmentation_masks(torch.tensor(np.array(img)).permute(2, 0, 1), mask.squeeze(1))\n",
    "    for img, mask in zip(images_list, boolean_masks)\n",
    "]\n",
    "ts.show(images_with_masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
