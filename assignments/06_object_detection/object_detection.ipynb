{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a135e5f4",
   "metadata": {},
   "source": [
    "## FHNW bverI - HS2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c6b40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f10f19697935a18db955753fdc25539f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b703d4d-0ab2-41cc-b8c4-3f42d2409dc8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56ba5caee0003f670d9dd680ee825fb",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Object-Detection\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- Objekt Klassifikation und Lokalisation: An einem einfachen Beispiel verstehen wie Objekte lokalisiert werden können.\n",
    "- Object Detection: Anwenden & verstehen von Pre-Trained Modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece4df1-36bf-4298-804d-c67b617e6a3f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Im Folgenden installieren und laden wir die benötigten Python packages. Danach setzten wir die Pfade für den Zugriff auf Daten und spezifizieren einen Output-Folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567ca16-601c-48d9-b4ca-ca2af6e5f228",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b6560-ea46-4ead-8649-069e7b89f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3a787-ee23-416e-91f8-115950b0b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f747d-65ac-48b3-8a0a-3adb81297871",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a184d2-43af-4b7a-bc03-776ecc8f336b",
   "metadata": {},
   "source": [
    "Modifizieren Sie die folgenden Pfade bei Bedarf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daafcf9-f5b7-46ea-9579-d2ebfa8725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DATA_PATH = Path('/content/drive/MyDrive/bverI/data')\n",
    "else:\n",
    "    DATA_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50464da-648c-4d0b-8471-aad55e6ed9e9",
   "metadata": {},
   "source": [
    "Install packages not in base Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8624f2-29f3-4b9f-9f51-69daff40350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.system(\"pip install torchshow torchinfo gdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b675f9-dcc4-4f25-8d3a-681178d79ceb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b21fd9e7063b6e0bf863573ec8845a0c",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96e630-7583-4979-981e-a1ef8dc62efb",
   "metadata": {},
   "source": [
    "## Object Klassifikation und Lokalisierung\n",
    "\n",
    "In dieser Aufgabe machen wir es uns etwas einfacher: Wir klassifizieren und lokalisieren genau ein Objekt pro Bild. Dies soll zeigen, wie Objetc Detection im Ansatz funktioniert und soll die Schwierigkeit der Problemstell verdeutlichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f171d3-fd72-40b1-ba2a-4d70e44c01a9",
   "metadata": {},
   "source": [
    "### Datensatz\n",
    "\n",
    "Wir erstellen uns einen künstlichen Datensatz. Dieser erstellt on-the-fly Bilder und Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cba06-2b73-43cc-9132-68f2ba4380a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, img_size=256, seed=123, max_number_of_shapes_per_image=5, background=\"random\", transforms=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.max_number_of_shapes_per_image = max_number_of_shapes_per_image\n",
    "        self.classes = [\"circle\", \"rectangle\", \"triangle\"]\n",
    "        self.class_map = {k: i for i, k in enumerate(self.classes)}\n",
    "        self.background = background\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def draw_random_circle(self, img, annotations):\n",
    "        # Generate a random color for the circle\n",
    "        color = tuple(int(value) for value in self.rng.integers(50, 255, size=3))\n",
    "\n",
    "        # Generate random coordinates for the center of the circle\n",
    "        center = (self.rng.integers(0, self.img_size), self.rng.integers(0, self.img_size))\n",
    "\n",
    "        # Generate a random radius for the circle (between 5% and 20% of img_size)\n",
    "        radius = int(self.rng.uniform(0.05 * self.img_size, 0.2 * self.img_size))\n",
    "\n",
    "        # Draw the filled circle on the image\n",
    "        cv2.circle(img, center, radius, color, -1)\n",
    "\n",
    "        # Calculate the bounding box for the circle (xmin, ymin, xmax, ymax)\n",
    "        bbox = (center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius)\n",
    "\n",
    "        # Append the annotation to the list with the class \"circle\"\n",
    "        annotations.append({\"class\": self.class_map['circle'], \"box\": bbox})\n",
    "\n",
    "    def draw_random_rectangle(self, img, annotations):\n",
    "        # Generate a random color\n",
    "        color = tuple(int(value) for value in self.rng.integers(50, 255, size=3))\n",
    "\n",
    "        # Generate random coordinates for the top-left corner of the rectangle (xmin, ymin)\n",
    "        xmin = self.rng.integers(0, int(self.img_size * 0.8))\n",
    "        ymin = self.rng.integers(0, int(self.img_size * 0.8))\n",
    "\n",
    "        # Generate random width and height for the rectangle\n",
    "        width = int(self.rng.uniform(0.05 * self.img_size, 0.2 * self.img_size))\n",
    "        height = int(self.rng.uniform(0.05 * self.img_size, 0.2 * self.img_size))\n",
    "\n",
    "        # Calculate the coordinates for the bottom-right corner of the rectangle (xmax, ymax)\n",
    "        xmax = xmin + width\n",
    "        ymax = ymin + height\n",
    "\n",
    "        # Draw the filled rectangle on the image\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, -1)\n",
    "\n",
    "        # Define the bounding box (xmin, ymin, xmax, ymax)\n",
    "        bbox = (xmin, ymin, xmax, ymax)\n",
    "\n",
    "        # Append the annotation to the list with the class \"rectangle\"\n",
    "        annotations.append({\"class\": self.class_map[\"rectangle\"], \"box\": bbox})\n",
    "\n",
    "    def draw_random_triangle(self, img, annotations):\n",
    "        # Generate a random color\n",
    "        color = tuple(int(value) for value in self.rng.integers(50, 255, size=3))\n",
    "\n",
    "        # Generate a random point for the top-left vertex of the triangle\n",
    "        pt1 = (\n",
    "            self.rng.integers(0, int(self.img_size * 0.8)),\n",
    "            self.rng.integers(0, int(self.img_size * 0.8))\n",
    "        )\n",
    "\n",
    "        # Generate random width and height for the triangle\n",
    "        width = int(self.rng.uniform(0.05 * self.img_size, 0.2 * self.img_size))\n",
    "        height = int(self.rng.uniform(0.05 * self.img_size, 0.2 * self.img_size))\n",
    "\n",
    "        # Calculate the coordinates for the other two points of the triangle\n",
    "        pt2 = (pt1[0] + width, pt1[1])\n",
    "        pt3 = (int((pt1[0] + pt2[0]) / 2), pt1[1] - height)\n",
    "\n",
    "        # Define the triangle points as a numpy array\n",
    "        triangle_points = np.array([pt1, pt2, pt3])\n",
    "\n",
    "        # Draw the filled triangle on the image\n",
    "        cv2.drawContours(img, [triangle_points], 0, color, -1)\n",
    "\n",
    "        # Calculate the bounding box (xmin, ymin, xmax, ymax)\n",
    "        x_coords = [pt[0] for pt in [pt1, pt2, pt3]]\n",
    "        y_coords = [pt[1] for pt in [pt1, pt2, pt3]]\n",
    "        bounding_box = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n",
    "\n",
    "        # Append the annotation to the list with the class \"triangle\"\n",
    "        annotations.append({\"class\": self.class_map[\"triangle\"], \"box\": bounding_box})\n",
    "\n",
    "    def _clip_bounding_box_to_image(self, annotations):\n",
    "        # Clip boxes to image size\n",
    "        box_unclipped = torch.tensor(annotations[-1]['box']).reshape(1, -1)\n",
    "        box_clipped = torchvision.ops.clip_boxes_to_image(\n",
    "            box_unclipped,\n",
    "            (self.img_size - 1, self.img_size - 1)\n",
    "        ).squeeze(0)\n",
    "        annotations[-1]['box'] = [int(x) for x in box_clipped]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.background == 'random':\n",
    "            img = self.rng.integers(0, 256, (self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "        elif self.background == 'white':\n",
    "            img = np.ones((self.img_size, self.img_size, 3), np.uint8) * 255  # White background\n",
    "\n",
    "        annotations = []\n",
    "\n",
    "        num_shapes = self.rng.integers(1, self.max_number_of_shapes_per_image + 1)  # Random number of shapes per image\n",
    "        for _ in range(num_shapes):\n",
    "            choice = self.rng.choice([\"circle\", \"rectangle\", \"triangle\"])\n",
    "            if choice == \"circle\":\n",
    "                self.draw_random_circle(img, annotations)\n",
    "            elif choice == \"rectangle\":\n",
    "                self.draw_random_rectangle(img, annotations)\n",
    "            else:\n",
    "                self.draw_random_triangle(img, annotations)\n",
    "            self._clip_bounding_box_to_image(annotations)\n",
    "\n",
    "        # Convert image to PyTorch tensor\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        labels = {'class': [x['class'] for x in annotations], 'box': [x['box'] for x in annotations]}\n",
    "\n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3b50f-0e11-4d3b-8654-5eece0a52914",
   "metadata": {},
   "source": [
    "Wir schauen uns den Datensatz nun etwas genauer an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646e1df-8a3c-408d-b959-25cb32518d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as FT\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision import transforms\n",
    "\n",
    "dataset = ShapeDataset(\n",
    "    img_size=64,\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    background=\"white\",\n",
    "    transforms=transforms.ToTensor())\n",
    "\n",
    "for i, (img, annotations) in enumerate(dataset):\n",
    "    img = (img * 255.0).to(torch.uint8)\n",
    "    boxes = torch.tensor(annotations['box'])\n",
    "    img_with_box = draw_bounding_boxes(image=img, boxes=boxes)\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    pil_image = F.to_pil_image(img_with_box)\n",
    "    _ = ax.imshow(pil_image)\n",
    "    _ = ax.set_xticks([])\n",
    "    _ = ax.set_yticks([])\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4446653-6ef3-4a09-ab9c-e423704c1847",
   "metadata": {},
   "source": [
    "**FRAGE:** Was sieht man im Ouptut oben? Was beinhaltet der Datensatz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f765aad-f5e4-4ebb-bf61-9c948569c321",
   "metadata": {},
   "source": [
    "### Trainings-Datensatz erstellen\n",
    "\n",
    "Nun erstellen wir einen Trainings-Datensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d84c4-f085-4500-bad4-cec18d2c07ef",
   "metadata": {},
   "source": [
    "Als erstes schauen wir uns den Output des `DataLoader` Objektes an. Damit muss unser Modell umgehen können.\n",
    "\n",
    "Überprüfen Sie die Outputs auf Datentyp und Shape.\n",
    "\n",
    "`collate_fn` definiert wie Samples gebatcht werden: [Link](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aa212-8186-4b5b-b9f5-45b35d02b570",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0236bfea8f9179b6ad099a21e57910c5",
     "grade": true,
     "grade_id": "cell-ab655ef85a9c58c5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5,1.0)])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    return torch.stack(images), annotations\n",
    "\n",
    "dataset = ShapeDataset(\n",
    "    img_size=64,\n",
    "    num_samples=1000,\n",
    "    background=\"white\",\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    transforms=transform)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "images, annotations = next(iter(dataloader))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76341cc3-0036-4c03-891d-ade8080aa718",
   "metadata": {},
   "source": [
    "**Frage:** Wie sieht der Output des `ShapeDataset` mit einem `DataLoader` aus? Was sind die Schwierigkeiten?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066c867-8209-4488-acc1-679324dc740c",
   "metadata": {},
   "source": [
    "### Architektur Definieren\n",
    "\n",
    "Wir definieren nun eine Architektur, die aus drei Komponenten besteht:\n",
    "\n",
    "- Backbone: Ein CNN für die Feature-Extraktion\n",
    "- Classification-Head: Modelliert die Klassifikation\n",
    "- Bounding-Box-Regression-Head: Modelliert die Bounding-Box Regression\n",
    "\n",
    "Erstellen Sie den Backbone mit 3 Convolutional Layers. \n",
    "\n",
    "- Conv Layer 1: 16 Filters, Stride 1\n",
    "- Conv Layer 2: 32 Filters, Stride 2\n",
    "- Conv Layer 3: 64 Filters, Stride 2\n",
    "\n",
    "Ergänzen Sie die Layers für die Heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f429a-0b52-4e40-ac52-26143a9003d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63669091f2df0a24b0b4d09f8c7d43ba",
     "grade": true,
     "grade_id": "cell-61e829411a3557bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self, input_shape=(64, 64), output_features=128):\n",
    "        super(CNNBackbone, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        global_stride = 4\n",
    "        cnn_features = 64 * (input_shape[0] // global_stride) * (input_shape[1] // global_stride)\n",
    "        self.fc1 = nn.Linear(cnn_features, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_classes, num_input_features):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        # self.fc2_class = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, x):\n",
    "        class_logits = self.fc2_class(x)\n",
    "        return class_logits\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, num_input_features):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        # self.fc2_bb = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bb_coords = self.fc2_bb(x)\n",
    "        return bb_coords\n",
    "\n",
    "class ObjectClassificationAndLocalization(nn.Module):\n",
    "    def __init__(self, input_shape, num_features, num_classes):\n",
    "        super(ObjectClassificationAndLocalization, self).__init__()\n",
    "        self.backbone = CNNBackbone(input_shape=input_shape, output_features=num_features)\n",
    "        self.classification_head = ClassificationHead(num_classes, num_features)\n",
    "        self.detection_head = DetectionHead(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        class_scores = self.classification_head(x)\n",
    "        bb_coords = self.detection_head(x)\n",
    "        return class_scores, bb_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb7c1b-e6fb-45f3-bdb1-aadb0cc5d44c",
   "metadata": {},
   "source": [
    "### Initialize Modell, Lossfunktion und Optimizer\n",
    "\n",
    "Initialisieren Sie ihr Modell. Definieren Sie Loss-Funktionen (separate für Klassifikation und Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd437c-c504-42c7-a9b2-07e3dba911ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07063f6b74c2502bfd0c2688883f37de",
     "grade": true,
     "grade_id": "cell-b45f32300f92c0db",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net = ObjectClassificationAndLocalization(\n",
    "    input_shape=(64, 64),\n",
    "    num_features=128,\n",
    "    num_classes=3)\n",
    "\n",
    "# loss_fn_class = nn.\n",
    "# loss_fn_bbx = nn.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ac37b-9740-4a8f-a10d-3780290dc957",
   "metadata": {},
   "source": [
    "### Modell Training\n",
    "\n",
    "Nun trainieren wir das Modell für 30 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ac9ea-b009-43d5-9c6f-1462bdfc79f2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5eee05f71a9b1febadb0e94b2ddd3b1a",
     "grade": true,
     "grade_id": "cell-992747d62f6b7322",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_cls_loss = 0.0\n",
    "    running_bbx_loss = 0.0\n",
    "    for step, (images, annotations) in enumerate(dataloader):\n",
    "        labels_class = torch.tensor([sample['class'][0] for sample in annotations])\n",
    "        labels_bb = torch.tensor([sample['box'][0] for sample in annotations])\n",
    "\n",
    "        # scale bb labels\n",
    "        labels_bb_scaled = labels_bb / images.shape[2]\n",
    "\n",
    "        # Forward pass\n",
    "        class_scores, bb_coords = net(images)\n",
    "\n",
    "        # compute classification and regression losses\n",
    "        # loss_class = ...\n",
    "        # loss_bb = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Total loss (you can adjust weights for classification and regression losses)\n",
    "        # total_loss = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_cls_loss += loss_class.item()\n",
    "        running_bbx_loss += loss_bb.item()\n",
    "        running_loss += (loss_class.item() + loss_bb.item())\n",
    "\n",
    "    print(f\"epoch: {epoch + 1:02d} - step: {step + 1:5d} - total loss: {running_loss / (step+1):.3f} \\\n",
    "        Class Loss: {running_cls_loss  / (step+1):.4f} BB Loss: {running_bbx_loss / (step+1) :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625f214-1dd9-4323-92fb-35eff8ab2853",
   "metadata": {},
   "source": [
    "**Frage:** Was fällt auf wenn Sie die Loss-Werte betrachten? Was können Sie dagegen tun?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca91806-0094-466b-a423-5855349112a1",
   "metadata": {},
   "source": [
    "### Modell Evaluation\n",
    "\n",
    "Nun evaluieren wir das Modell (mit einfachen Mitteln)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b2aae-dd3a-4b95-b1bd-94e7be18f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ShapeDataset(\n",
    "    background=\"white\",\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    img_size=64,\n",
    "    num_samples=256,\n",
    "    transforms=transform)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a5639-7fe3-4557-b9e6-afd071f6f350",
   "metadata": {},
   "source": [
    "Hier schauen wir uns die Klassifikations-Performance an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a7a42-f4d8-45f9-b722-c3f3d6890ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    net = net.eval()\n",
    "    class_scores, bb_coords = net(test_images)\n",
    "\n",
    "y_class_pred = torch.argmax(class_scores, 1).numpy()\n",
    "y_class_true = np.array([sample['class'][0] for sample in test_labels])\n",
    "\n",
    "for i, (y_pred, y_true) in enumerate(zip(y_class_pred, y_class_true)):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(f\"Predicted: {test_dataset.classes[y_pred]} True: {test_dataset.classes[y_true]}\")\n",
    "    \n",
    "\n",
    "print(f\"Accuracy: {sum(y_class_pred == y_class_true) / len(y_class_true)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e0bae-6620-43d0-b16d-f1fea50d7f03",
   "metadata": {},
   "source": [
    "Nun visualisieren wir die Predictions und Ground Truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c38bc-49f4-4a66-84ce-d41544b874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ShapeDataset(\n",
    "    img_size=64,\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    background=\"white\",\n",
    "    transforms=transform)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "for i, (img, annotations) in enumerate(test_dataloader):\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        class_scores, bb_coords_scaled = net(img)\n",
    "        bb_coords = bb_coords_scaled * img.shape[2]\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    img = img.squeeze(0)\n",
    "        \n",
    "    img = (img * 2 * 255.0).to(torch.uint8)\n",
    "    boxes = torch.tensor(annotations[0]['box'])\n",
    "    img_with_box = draw_bounding_boxes(image=img, boxes=boxes, colors=\"red\")\n",
    "\n",
    "    # Plot Prediction\n",
    "    img_with_box = draw_bounding_boxes(image=img_with_box, boxes=bb_coords, colors=\"green\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    pil_image = F.to_pil_image(img_with_box)\n",
    "    _ = ax.imshow(pil_image)\n",
    "    # Remove x and y axis ticks\n",
    "    _ = ax.set_xticks([])\n",
    "    _ = ax.set_yticks([])\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d0f91-6324-4932-9fbe-1dc763c72c90",
   "metadata": {},
   "source": [
    "**Frage:** Rein qualitativ: Wie finden Sie die Klassifikationen und Lokalisationen ihres Modells? Wo könnte man sich verbessern?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ca539-729a-401c-a8e6-13b6e391861b",
   "metadata": {},
   "source": [
    "Im Folgenden schauen wir uns die Performance der Detektion an. Dazu berechnen wir die IoU.\n",
    "\n",
    "Ergänzen Sie den Code. Verwenden Sie [torchvision.ops.box_iou](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db1e2-df8f-44c4-9fe1-a7e940884ba1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76de1051304a83a84d22b32caace223b",
     "grade": true,
     "grade_id": "cell-8f488a9b108684ea",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = ShapeDataset(\n",
    "    img_size=64,\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    background=\"white\",\n",
    "    transforms=transform)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "for i, (img, annotations) in enumerate(test_dataloader):\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        class_scores, bb_coords_scaled = net(img)\n",
    "        predicted_box = bb_coords_scaled * img.shape[2]\n",
    "\n",
    "    gt_box = torch.tensor(annotations[0]['box']).reshape(1, -1)\n",
    "\n",
    "    # iou = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    print(f\"IOU: {iou:.2f}\")\n",
    "    \n",
    "    # Plot Ground Truth\n",
    "    img = img.squeeze(0)\n",
    "        \n",
    "    img = (img * 2 * 255.0).to(torch.uint8)\n",
    "    boxes = torch.tensor(annotations[0]['box'])\n",
    "    img_with_box = draw_bounding_boxes(image=img, boxes=boxes, colors=\"red\")\n",
    "\n",
    "    # Plot Prediction\n",
    "    img_with_box = draw_bounding_boxes(image=img_with_box, boxes=predicted_box, colors=\"green\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    pil_image = F.to_pil_image(img_with_box)\n",
    "    _ = ax.imshow(pil_image)\n",
    "    # Remove x and y axis ticks\n",
    "    _ = ax.set_xticks([])\n",
    "    _ = ax.set_yticks([])\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810200-6ec1-4ba9-9ac6-b99452867eac",
   "metadata": {},
   "source": [
    "**Frage:** Welche IoU Werte finden Sie akzeptabel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c262042-a682-4253-896d-d587953cf010",
   "metadata": {},
   "source": [
    "**Frage:** Was müsste man alles anpassen, damit man Object Detection durchführen kann? (mehrere Objekte pro Bild)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fb9a2-5d8f-42c9-9835-e9ac81024e7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3412a6a68a9f5aa6b49698b6aba396d4",
     "grade": false,
     "grade_id": "cell-edcdcb36b38944ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Pre-Trained _Faster R-CNN_\n",
    "\n",
    "In dieser Aufgabe werden wir ein vortrainiertes Object Detection Modell der Familie der _Faster R-CNNs_ einsetzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb746ed-86b4-4203-808a-767ef046a91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b3141b331b9fc9a782161431f61f110",
     "grade": false,
     "grade_id": "cell-1ad02c611cb6b0d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Daten\n",
    "\n",
    "Lesen Sie die folgenden Bilder ein mit `PIL.Image`. Schauen Sie die Bilder an und überlegen Sie sich wie gut Object-Detection funktionieren könnte.\n",
    "\n",
    "```\n",
    "DATA_PATH.joinpath(\"dogs.jpg\")\n",
    "DATA_PATH.joinpath(\"ducks.jpeg\")\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb29c5-7b7c-4018-aee6-5ef7ade2b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "files = [\n",
    "    {'id': '18zuHwfojUUpmkrQttEtuaNW-MQ0QOoAH',  'name': 'ducks.jpg'},\n",
    "    {'id': '1-UWVWqTpE80Qxh36hPuKkuQZj5BT3hXr', 'name': 'dogs.jpg'}\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    url = f\"https://drive.google.com/uc?id={file['id']}\"\n",
    "    download_path = DATA_PATH / file['name']\n",
    "    if not download_path.exists():\n",
    "        gdown.download(url, str(download_path), quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84076903-a758-45e3-8c19-f33abeed0e81",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "601867454fd1da7d6544c16319203328",
     "grade": true,
     "grade_id": "cell-fe920be22a3799de",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519ef56-3c75-4ab9-b9c9-74d87f9683a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ca9743070a37cad92b78f73dead6bf8",
     "grade": false,
     "grade_id": "cell-b62acd4ad33420df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Modell Laden\n",
    "\n",
    "Laden Sie ein vortrainiertes Modell der _Faster R-CNN_ Familie von [torchvision](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection). Eine Möglichkeit ist z.B. das `fasterrcnn_mobilenet_v3_large_320_fpn`, welches Resourcen-schonend ist. Wenn Sie bessere Performance möchten, können Sie gerne ein anderes wählen.\n",
    "\n",
    "\n",
    "Initialisieren Sie das Modell und setzten Sie es in den `eval` Mode. Setzten Sie `box_score_thresh` auf einen Wert zwischen 0.5 und 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0a5e-95a0-4914-b1da-802f79724cd4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ec703a9e44d8b11d40720eca0862511",
     "grade": true,
     "grade_id": "cell-f757fea1c417faf8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "    FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "  )\n",
    "\n",
    "\n",
    "Model = fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61f022-78fb-4b4f-a350-7645b20aba33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "662ba6858f4b47b86cf11fb9084ebeb1",
     "grade": false,
     "grade_id": "cell-bc2e5e0d131aa594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Modell Anwenden\n",
    "\n",
    "Benutzen Sie Funktion `inference()` um Predictions für ein Bild zu generieren. Schauen Sie sich dazu folgendes Beispiel an: https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\n",
    "\n",
    "Erstellen Sie danach Predictions für \"dogs.jpg\" und inspizieren Sie den Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e0874-33cd-4ddc-8322-fb8593293d1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9097a2e1295e9d5afa9d9ddd854870a9",
     "grade": true,
     "grade_id": "cell-eafdeb3e614c1507",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_single(img, model, preprocess):\n",
    "    \"\"\" Inference on a single image\n",
    "        Args:\n",
    "            img: (C, H, W) torch.tensor\n",
    "            model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "            preprocess: function to pre-process image batch for the model\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "    \n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "    return model(image_processed)[0]\n",
    "\n",
    "from torchvision.transforms import functional as FT\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9c3ae-aef5-4918-8350-817d60d85af1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb42a3009f14465170384349394189f",
     "grade": false,
     "grade_id": "cell-cbdc45817692565a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Visualisieren Sie die Predictions mit [torchvision.utils.draw_bounding_boxes](torchvision.utils.draw_bounding_boxes). Visualisieren Sie die Labels, sowie die Confidence-Scores der Predictions zusammen mit den Bounding-Boxes.\n",
    "\n",
    "Die Labels finden Sie in `weights.meta[\"categories\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331a863-9f13-4f64-b99d-7283ce457fb3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff2222e7fe1c9920cb93c9938c066f30",
     "grade": true,
     "grade_id": "cell-a1ff57e859223906",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def draw_boxes(image, predictions, categories):\n",
    "    \"\"\" Draw Boxes from Predictions \n",
    "        Args:\n",
    "            image: The input image torch.tensor\n",
    "            predictions: Output of inference()\n",
    "            categories: List of category labels\n",
    "        Returns:\n",
    "            PIL.Image\n",
    "    \"\"\"\n",
    "    labels = [f\"{categories[i]} ({s * 100:.2f} %)\" for i, s in zip(predictions[\"labels\"], predictions[\"scores\"])]\n",
    "\n",
    "    box = draw_bounding_boxes(\n",
    "        image, boxes=predictions[\"boxes\"],\n",
    "        labels=labels, width=5,\n",
    "        colors=\"red\")\n",
    "    img = box.detach()\n",
    "    return FT.to_pil_image(img)\n",
    "    #im = Image.fromarray(im.permute(1, 2, 0).numpy())\n",
    "    #return im\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431c7d9-f1dd-447e-b421-e2a3b2517e91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3f539e529c076dd17a0360b4d0d59ee",
     "grade": false,
     "grade_id": "cell-d27167e6ad5bed75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Initialisieren Sie das Modell neu und wählen Sie einen tieferen Wert für [box_score_thresh](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py). \n",
    "\n",
    "Erstellen Sie danach Predictions für \"ducks.jpg\".\n",
    "\n",
    "Visualisieren Sie wieder die Boxen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c248fee-2e2a-444c-bc54-dde5d5aa2b8d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50b2b4ea0a4ba8f5090ee2626793071f",
     "grade": true,
     "grade_id": "cell-37ed2929b2adae96",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb6067-e046-40e4-aff9-8f1ee3eb10b7",
   "metadata": {},
   "source": [
    "Berechnen Sie die IoU für die gefundenen Boxen. Sie können folgende Funktion verwenden [torchvision.ops.box_iou](https://pytorch.org/vision/stable/generated/torchvision.ops.box_iou.html#torchvision.ops.box_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20218d8-ed5b-47db-925a-c709d27c4d27",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41fe601b79c6e00ef600119e9930b780",
     "grade": true,
     "grade_id": "cell-3dfb751c58d0c305",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e5a81-9862-4085-855d-98800933c3a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb7d668bda819059f6bdba9ce7df2295",
     "grade": false,
     "grade_id": "cell-b266289993f18851",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Nun schauen wir uns die Activation Maps vom Backbone-CNN an, welche in das RPN geht. Verwenden Sie dazu die folgende Funktion und inspizieren Sie die Shape der Activation Map.\n",
    "\n",
    "Schauen Sie sich die Activation Maps von beiden Beispiel-Bildern an. Was stellen Sie fest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31691f6-2e5c-49fc-bfc6-6fd6c2491eca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c71a13f0f470724f3cae894ace9d89",
     "grade": true,
     "grade_id": "cell-89997b51c70d45d9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backbone(img, model, preprocess):\n",
    "    \"\"\" Get Features from the Backbone Network\n",
    "        Args:\n",
    "            img: (C, H, W) torch.tensor\n",
    "            model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "            preprocess: function to pre-process image batch for the model\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "    features = model.backbone(image_processed)\n",
    "    return features['0']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff6dc6-304d-4260-b133-b9c78f51f5c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a931547d829fc376750e6b4489dec73",
     "grade": false,
     "grade_id": "cell-55115421343d1fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Nun schauen wir uns den Output des RPNs an. Vergleichen Sie wieder die beiden Bilder.\n",
    "\n",
    "Setzen Sie die Modell-Parameter: `rpn_score_thresh` und  `rpn_post_nms_top_n_test` und schauen Sie verschiedene Werte an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60120e6-e264-4ef9-8c4b-1c185ac47d7b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09d0d3b0b6c47ce1e4b9778d1e406864",
     "grade": true,
     "grade_id": "cell-f4bc3ac14d76daf4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rpn(img, model, preprocess):\n",
    "    \"\"\" Get Region Proposals\n",
    "        Args:\n",
    "            img: (C, H, W) torch.tensor\n",
    "            model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "            preprocess: function to pre-process image batch for the model\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "    \n",
    "    images, targets = model.transform(image_processed, targets=None)\n",
    "    features = model.backbone(image_processed)\n",
    "    proposals, proposal_losses = model.rpn(images, features, targets=targets)\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in image_batch:\n",
    "        val = img.shape[-2:]\n",
    "        torch._assert(\n",
    "            len(val) == 2,\n",
    "            f\"expecting the last two dimensions of the Tensor to be H and W instead got {img.shape[-2:]}\",\n",
    "        )\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "    proposals = model.transform.postprocess([{'boxes': proposals[0]}], images.image_sizes, original_image_sizes)\n",
    "\n",
    "    return proposals\n",
    "\n",
    "\n",
    "def draw_proposals(image, proposals):\n",
    "    \"\"\" Draw Boxes from Predictions \n",
    "        Args:\n",
    "            image: The input image torch.tensor\n",
    "            predictions: Output of inference()\n",
    "            categories: List of category labels\n",
    "        Returns:\n",
    "            PIL.Image\n",
    "    \"\"\"\n",
    "\n",
    "    box = draw_bounding_boxes(\n",
    "        image, boxes=proposals,width=5,\n",
    "        colors=\"red\")\n",
    "    im = box.detach()\n",
    "    im = Image.fromarray(im.permute(1, 2, 0).numpy())\n",
    "    return im\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
